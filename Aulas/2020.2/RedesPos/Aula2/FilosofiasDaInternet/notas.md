# Filosofias e Princípios de Projeto da Internet

Embora ainda não tenhamos entrado em detalhes sobre o funcionamento da Internet, já deve ser possível perceber que se trata de um sistema bastante complexo. É uma rede com **bilhões de dispositivos**, esses dispositivos são **heterogêneos**, administrados por **entidades diferentes** que têm liberdade para seguir **políticas de administração diferentes**. Além disso, o **volume** de informação que trafega na rede a cada **segundo** é enorme. Considere, ainda, o fato de que esse tráfego viabiliza hoje uma série de **atividades econômicas e sociais fundamentais** para a sociedade moderna.

Todos esses fatores exercem uma grande pressão no projeto da Internet e nas soluções e protocolos utilizados para implementá-lo. Em particular, por se tratar de uma rede tão heterogênea --- tanto em termos de *hardware* quanto em termos de *administração* --- existe uma necessidade de uniformização, *i.e.*, da definição de alguns princípios básicos que devem ser seguidos por todos os participantes da rede.

Na aula anterior, discutimos brevemente uma das ferramentas utilizadas para essa uniformização: o estabelecimento de protocolos de rede, determinando como esses diferentes dispositivos devem se comunicar uns com os outros. No entanto, apenas estabelecer regras rígidas para como a comunicação deve ser realizada na Internet não é suficiente. Ao contrário, os protocolos usados na Internet precisam ser flexíveis, no sentido de não assumir muita coisa a respeito dos dispositivos da rede. Para entender o porquê, considere um protocolo hipotético que só funcione de maneira correta em dispositivos conectados ao restante da Internet por enlaces de, ao menos, 1 Gb/s. Segundo um relatório da Akamai --- uma grande empresa do ramo de distribuição de conteúdo na Internet ---, em 2016 o país com maior velocidade média de acesso à Internet no mundo era a Coreia do Sul, onde, em média, usuários finais se conectavam a 28,6 Mb/s --- muito abaixo dos 1 Gb/s requeridos pelo nosso protocolo hipotético.

Assim, tal protocolo seria suportado por uma fração muito pequena da rede atual. Embora para certos protocolos isso possa ser aceitável --- por exemplo, jogos frequentemente impõem requisitos mínimos que nem sempre são atendidos pelos potenciais usuários ---, esse tipo de restrição não pode estar presente nos protocolos básicos da Internet --- justamente aqueles que garantem a interoperabilidade dos dispositivos heterogêneos.

Como veremos a seguir, esse tipo de consideração vem guiando decisões de projeto na Internet desde suas origens.

## O Argumento Fim-a-Fim

O *Internet Architecture Board* é um comitê que foi criado em 1979 pela DARPA para supervisionar e guiar o desenvolvimento arquitetural e de padrões na Internet. De 1981 a 1989, esse comitê foi dirigido por David Clark, atualmente um pesquisador no MIT. Durante esse período, Clark esteve diretamente envolvido com decisões de projeto fundamentais que tiveram --- e ainda tem --- impacto profundo no funcionamento da Internet.

Entre suas inúmeras contribuições, ele publicou em 1988 um artigo intitulado *"The Design Philosophy of the DARPA Internet Protocols"* que introduz o chamado _Argumento Fim-a-Fim_, um princípio que tem guiado o projeto da Internet ao longo dos anos[^ArgFimAFimExceções]. Resumidamente, o Argumento Fim-a-Fim defende que uma funcionalidade só pode ser implementada de forma **correta e completa** se isso for feito com o auxílio das aplicações executadas nas **pontas do sistema de comunicação**.

[^ArgFimAFimExceções]: Embora existam várias exceções, conforme veremos ao longo deste curso.

No artigo, Clark provê uma série de exemplos para corroborar essa afirmação. Um deles é o de uma aplicação de transmissão de arquivos entre dois computadores. Os computadores estão conectados por algum tipo de rede de comunicação baseada em comutação de pacotes. Isso significa que o arquivo --- potencialmente grande --- será quebrado em um ou mais pacotes. Além disso, Clark assume que pacotes podem ser corrompidos ao serem transmitidos pelos enlaces, o que, de fato, ocorre em redes reais. Dadas todas essas hipóteses, nossa aplicação tem como objetivo fazer com que o arquivo chegue *íntegro* ao destinatário. Integridade, aqui, significa que a cópia do arquivo gravada em memória no computador de destino será idêntica, bit a bit, à cópia original no computador de origem.

Como uma tentativa inicial, vamos contrariar o Argumento Fim-a-Fim e supor que a funcionalidade de garantir a integridade dos dados seja implementada totalmente pela rede. Ou seja, a rede conterá mecanismos que garantem que a sequência de bits entregue ao computador de destino da aplicação será idêntica à sequência armazenada na memória do computador de origem. Como pacotes podem ser corrompidos durante a transmissão pelos enlaces, isso significa que, de alguma maneira, todo *host*/comutador precisa verificar a integridade dos pacotes recebidos[^VerificaçãoDeIntegridade]. Caso o pacote recebido não seja íntegro, o *host*/comutador requisita uma _retransmissão_. Além disso, vimos também que na Internet pacotes podem chegar _fora de ordem_ --- por exemplo, por mudanças no roteamento. Assim, de alguma forma, a rede deve evitar essa situação e garantir que os pacotes sejam entregues ao destinatário na exata ordem em que saíram da origem.

[^VerificaçãoDeIntegridade]: Mecanismos de verificação de integridade de pacotes existem e são bastante utilizados em redes de computadores. Por ora, não entraremos em detalhes como eles funcionam. No terceiro capítulo desse curso veremos um exemplo. 

Esse conjunto de mecanismos é suficiente para garantir a integridade do arquivo no receptor? Infelizmente, não. E por vários motivos. Por exemplo:

- Quem garante que o arquivo estava íntegro quando *saiu* do *host* de origem?
	- A leitura do arquivo a partir do disco pode ter sofrido alguma corrupção.
- Quem garante que não houve alguma falha na verificação de integridade de algum pacote em algum salto?
- Quem garante que as implementações dos comutadores/*hosts* estão absolutamente corretas (*i.e.*, não há bugs)?

Todos esses fatores fazem sugerem que, em última instância, o receptor ainda deve realizar uma verificação de integridade do arquivo completo recebido. Nesse caso, devemos nos perguntar: se a implementação da "garantia" de integridade na rede não é 100% eficaz, é desejável que essa funcionalidade seja implementada? Possivelmente não, dado que:

- A implementação dessa funcionalidade adiciona complexidade à rede.
	- Um sistema já bastante complexo se torna ainda mais difícil de projetar, implementar e manter.
- Tal implementação assume certas características dos comutadores intermediários.
	- Por exemplo, todos devem ter capacidade de processamento e memória suficientes para executar o algoritmo de verificação e a funcionalidade de retransmissão.
- Ela também causa efeitos colaterais para certas aplicações.
	- Retransmissões aumentam o tempo até a entrega de um pacote. Algumas aplicações não necessitam de integridade, mas são prejudicadas por atrasos excessivos na entrega dos dados.

Consideremos agora a aplicação do Argumento Fim-a-Fim ao projeto desse sistema. Basicamente, aceitaremos o fato de que o *host* de destino ainda precisará ele próprio verificações de integridade dos dados recebidos e abriremos mão de manter essa funcionalidade também na rede. Nesse caso, os bits do arquivo podem chegar corrompidos --- ou faltando --- no destinatário que (espera-se) detectará a falha de integridade e requisitará retransmissões diretamente à origem. Note que da mesma forma que a verificação de integridade pode falhar na implementação pela rede, ela também pode falhar quando executada pelo *host* de destino --- *e.g.*, pode haver um *bug* no código de verificação ou uma falha de *hardware* durante a execução. Entretanto, temos como vantagem a ausência de redundância na implementação da funcionalidade: apenas o *host* precisa implementá-la, simplificando o projeto, implementação e manutenção da rede.

Repare que o Argumento Fim-a-Fim é uma **filosofia geral de projeto**, mas não uma verdade absoluta. Conforme veremos nessa disciplina --- e em Redes de Computadores II ---, é possível encontrar uma série de exceções a essa filosofia em protocolos e mecanismos utilizados na Internet. Em geral, essas exceções estão ligadas a benefícios de desempenho ao se implementar determinado mecanismo na rede, ao invés de delegá-lo (totalmente) as pontas da comunicação. Por exemplo, voltando à aplicação de transmissão de arquivos, um benefício de realizar a verificação de integridade salto a salto é que corrupções dos pacotes podem ser recuperadas mais rapidamente. Isso porque, no caso fim-a-fim, a detecção da corrupção e solicitação de uma nova cópia do pacote só acontecem quando esse chega ao destinatário, o que faz com que o tempo total para a versão íntegra chegar também aumente. Além disso, **recursos da rede podem ser desperdiçados** com a transmissão por vários enlaces de um pacote corrompido anteriormente. 

Assim, em certos **casos particulares** otimizações são realizadas em desacordo com o Argumento Fim-a-Fim. No entanto, o Argumento Fim-a-Fim é majoritariamente seguido no projeto arquitetural e de protocolos da Internet. Veremos inúmeros exemplos disso ao longo desse curso.

### Argumento Fim-a-Fim: Analogias

Embora o argumento Fim-a-Fim seja uma filosofia de projeto aplicável a redes de comunicação, é possível encontrar paralelos com conceitos aplicados em outras áreas. Por exemplo, na área de arquitetura de computadores, há o embate entre arquiteturas RISC (*Reduced Instruction Set Computer*) e CISC (*Complex Instruction Set Computer*). Uma arquitetura CISC implementa um conjunto grande de instruções de máquina, suprindo, assim, em *hardware* grande parte das operações tipicamente realizadas pelas aplicações. Já uma arquitetura RISC oferece um conjunto de instruções comparativamente menor, focando em oferecer um mínimo de operações em *hardware* que sejam suficientes para que as funcionalidades mais complexas sejam implementadas em *sofware*. Embora fuja ao escopo dessa disciplina discutir os méritos de cada abordagem, os defensores de arquiteturas RISC argumentam que o menor conjunto de instruções torna os processadores mais simples, menos susceptíveis a *bugs* e pode trazer benefícios de desempenho ao permitir implementações mais eficientes das instruções básicas oferecidas. Tais argumentos são bem similares aos usados para justificar o argumento Fim-a-Fim.

Outro exemplo é o princípio da Navalha de Occam. Esse princípio filosófico defende que, dadas duas explicações consistentes para um mesmo fenômeno, é preferível optar pela mais simples --- *i.e.*, a que contém o menor número de suposições. Esse princípio é frequentemente utilizado em ciência para a explicação de fenômenos observados. Uma das razões para isso é ideia intuitiva de que, quanto menor o número de hipóteses não testadas em uma explicação para um dado fenômeno, menor a probabilidade de que essa explicação venha a ser refutada futuramente --- por exemplo, por novas observações inconsistentes com a explicação ou através de contraprovas às hipóteses. Assim como no caso do Argumento Fim-a-Fim, opta-se pela simplicidade, em detrimento a complexidades adicionais injustificadas.

### Argumento Fim-a-Fim: Inteligência nas Bordas

Um corolário do Argumento Fim-a-Fim é a tendência na Internet de se **manter a complexidade nas bordas**. Dito de outra forma, o projeto da Internet busca manter o **núcleo da rede simples**, ao concentrar a **inteligência nas bordas**.

Como os elementos do núcleo da rede tendem a processar um volume muito maior de pacotes que os da borda, objetiva-se mantê-los o mais simples e especializados o possível, de forma que eles possam exercer essas poucas funcionalidades de maneira bastante eficiente. Em última análise, ao manter o núcleo da rede simples, aumentamos a capacidade de **escalabilidade** da Internet --- em termos de suportar demandas crescentes de usuários e tráfego. Já nas bordas, como o volume de pacotes processados é tipicamente menor, pode-se dar ao luxo de realizar tarefas mais complexas, já que a pressão por escalabilidade é mais baixa.

Embora posto nesses termos o conceito de inteligência nas bordas possa parecer uma opção óbvia, vale destacar que nem toda rede de comunicação é assim. De fato, outra rede de comunicação extremamente popular adota o paradigma exatamente oposto: na telefonia fixa, o núcleo concentra a enorme maioria das funcionalidades, enquanto os terminais --- aparelhos de telefone --- são bastante simples.

## O Princípio KISS

Outra filosofia geral de projeto amplamente adotada na Internet é o chamado _princípio KISS_ (do inglês, *Keep It Simple, Stupidy!*). Esse princípio foi originado na Marinha Americana na década de 1960, mas foi incorporado às filosofias que guiam o desenvolvimento da Internet desde os seus primórdios. O princípio KISS defende que sistemas _funcionam melhor_ quando são mantidos **simples**. Partindo dessa premissa, a simplicidade deve ser um objetivo do projeto. Assim, quaisquer complexidades que não sejam absolutamente necessárias aos requisitos do sistema deve ser evitadas.

O princípio KISS é aplicável a qualquer projeto de computação ou engenharia. No caso particular da Internet, ele se manifesta principalmente na forma de projetos de protocolos simples (relativamente falando). Quanto mais simples é um protocolo, mais fácil é sua implementação, depuração e extensão. Ao longo de todo o curso, veremos exemplos de opções por simplicidade em diversos protocolos amplamente utilizados na Internet.
