<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Filosofias de Projeto da Internet</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="filosofias-e-princípios-de-projeto-da-internet">Filosofias e Princípios de Projeto da Internet</h1>
<p>Embora ainda não tenhamos entrado em detalhes sobre o funcionamento da Internet, já deve ser possível perceber que se trata de um sistema bastante complexo. É uma rede com <strong>bilhões de dispositivos</strong>, esses dispositivos são <strong>heterogêneos</strong>, administrados por <strong>entidades diferentes</strong> que têm liberdade para seguir <strong>políticas de administração diferentes</strong>. Além disso, o <strong>volume</strong> de informação que trafega na rede a cada <strong>segundo</strong> é enorme. Considere, ainda, o fato de que esse tráfego viabiliza hoje uma série de <strong>atividades econômicas e sociais fundamentais</strong> para a sociedade moderna.</p>
<p>Todos esses fatores exercem uma grande pressão no projeto da Internet e nas soluções e protocolos utilizados para implementá-lo. Em particular, por se tratar de uma rede tão heterogênea — tanto em termos de <em>hardware</em> quanto em termos de <em>administração</em> — existe uma necessidade de uniformização, <em>i.e.</em>, da definição de alguns princípios básicos que devem ser seguidos por todos os participantes da rede.</p>
<p>Na aula anterior, discutimos brevemente uma das ferramentas utilizadas para essa uniformização: o estabelecimento de protocolos de rede, determinando como esses diferentes dispositivos devem se comunicar uns com os outros. No entanto, apenas estabelecer regras rígidas para como a comunicação deve ser realizada na Internet não é suficiente. Ao contrário, os protocolos usados na Internet precisam ser flexíveis, no sentido de não assumir muita coisa a respeito dos dispositivos da rede. Para entender o porquê, considere um protocolo hipotético que só funcione de maneira correta em dispositivos conectados ao restante da Internet por enlaces de, ao menos, 1 Gb/s. Segundo um relatório da Akamai — uma grande empresa do ramo de distribuição de conteúdo na Internet —, em 2016 o país com maior velocidade média de acesso à Internet no mundo era a Coreia do Sul, onde, em média, usuários finais se conectavam a 28,6 Mb/s — muito abaixo dos 1 Gb/s requeridos pelo nosso protocolo hipotético.</p>
<p>Assim, tal protocolo seria suportado por uma fração muito pequena da rede atual. Embora para certos protocolos isso possa ser aceitável — por exemplo, jogos frequentemente impõem requisitos mínimos que nem sempre são atendidos pelos potenciais usuários —, esse tipo de restrição não pode estar presente nos protocolos básicos da Internet — justamente aqueles que garantem a interoperabilidade dos dispositivos heterogêneos.</p>
<p>Como veremos a seguir, esse tipo de consideração vem guiando decisões de projeto na Internet desde suas origens.</p>
<h2 id="o-argumento-fim-a-fim">O Argumento Fim-a-Fim</h2>
<p>O <em>Internet Architecture Board</em> é um comitê que foi criado em 1979 pela DARPA para supervisionar e guiar o desenvolvimento arquitetural e de padrões na Internet. De 1981 a 1989, esse comitê foi dirigido por David Clark, atualmente um pesquisador no MIT. Durante esse período, Clark esteve diretamente envolvido com decisões de projeto fundamentais que tiveram — e ainda tem — impacto profundo no funcionamento da Internet.</p>
<p>Entre suas inúmeras contribuições, ele publicou em 1988 um artigo intitulado <em>“The Design Philosophy of the DARPA Internet Protocols”</em> que introduz o chamado <em>Argumento Fim-a-Fim</em>, um princípio que tem guiado o projeto da Internet ao longo dos anos<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Resumidamente, o Argumento Fim-a-Fim defende que uma funcionalidade só pode ser implementada de forma <strong>correta e completa</strong> se isso for feito com o auxílio das aplicações executadas nas <strong>pontas do sistema de comunicação</strong>.</p>
<p>No artigo, Clark provê uma série de exemplos para corroborar essa afirmação. Um deles é o de uma aplicação de transmissão de arquivos entre dois computadores. Os computadores estão conectados por algum tipo de rede de comunicação baseada em comutação de pacotes. Isso significa que o arquivo — potencialmente grande — será quebrado em um ou mais pacotes. Além disso, Clark assume que pacotes podem ser corrompidos ao serem transmitidos pelos enlaces, o que, de fato, ocorre em redes reais. Dadas todas essas hipóteses, nossa aplicação tem como objetivo fazer com que o arquivo chegue <em>íntegro</em> ao destinatário. Integridade, aqui, significa que a cópia do arquivo gravada em memória no computador de destino será idêntica, bit a bit, à cópia original no computador de origem.</p>
<p>Como uma tentativa inicial, vamos contrariar o Argumento Fim-a-Fim e supor que a funcionalidade de garantir a integridade dos dados seja implementada totalmente pela rede. Ou seja, a rede conterá mecanismos que garantem que a sequência de bits entregue ao computador de destino da aplicação será idêntica à sequência armazenada na memória do computador de origem. Como pacotes podem ser corrompidos durante a transmissão pelos enlaces, isso significa que, de alguma maneira, todo <em>host</em>/comutador precisa verificar a integridade dos pacotes recebidos<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Caso o pacote recebido não seja íntegro, o <em>host</em>/comutador requisita uma <em>retransmissão</em>. Além disso, vimos também que na Internet pacotes podem chegar <em>fora de ordem</em> — por exemplo, por mudanças no roteamento. Assim, de alguma forma, a rede deve evitar essa situação e garantir que os pacotes sejam entregues ao destinatário na exata ordem em que saíram da origem.</p>
<p>Esse conjunto de mecanismos é suficiente para garantir a integridade do arquivo no receptor? Infelizmente, não. E por vários motivos. Por exemplo:</p>
<ul>
<li>Quem garante que o arquivo estava íntegro quando <em>saiu</em> do <em>host</em> de origem?
<ul>
<li>A leitura do arquivo a partir do disco pode ter sofrido alguma corrupção.</li>
</ul></li>
<li>Quem garante que não houve alguma falha na verificação de integridade de algum pacote em algum salto?</li>
<li>Quem garante que as implementações dos comutadores/<em>hosts</em> estão absolutamente corretas (<em>i.e.</em>, não há bugs)?</li>
</ul>
<p>Todos esses fatores fazem sugerem que, em última instância, o receptor ainda deve realizar uma verificação de integridade do arquivo completo recebido. Nesse caso, devemos nos perguntar: se a implementação da “garantia” de integridade na rede não é 100% eficaz, é desejável que essa funcionalidade seja implementada? Possivelmente não, dado que:</p>
<ul>
<li>A implementação dessa funcionalidade adiciona complexidade à rede.
<ul>
<li>Um sistema já bastante complexo se torna ainda mais difícil de projetar, implementar e manter.</li>
</ul></li>
<li>Tal implementação assume certas características dos comutadores intermediários.
<ul>
<li>Por exemplo, todos devem ter capacidade de processamento e memória suficientes para executar o algoritmo de verificação e a funcionalidade de retransmissão.</li>
</ul></li>
<li>Ela também causa efeitos colaterais para certas aplicações.
<ul>
<li>Retransmissões aumentam o tempo até a entrega de um pacote. Algumas aplicações não necessitam de integridade, mas são prejudicadas por atrasos excessivos na entrega dos dados.</li>
</ul></li>
</ul>
<p>Consideremos agora a aplicação do Argumento Fim-a-Fim ao projeto desse sistema. Basicamente, aceitaremos o fato de que o <em>host</em> de destino ainda precisará ele próprio verificações de integridade dos dados recebidos e abriremos mão de manter essa funcionalidade também na rede. Nesse caso, os bits do arquivo podem chegar corrompidos — ou faltando — no destinatário que (espera-se) detectará a falha de integridade e requisitará retransmissões diretamente à origem. Note que da mesma forma que a verificação de integridade pode falhar na implementação pela rede, ela também pode falhar quando executada pelo <em>host</em> de destino — <em>e.g.</em>, pode haver um <em>bug</em> no código de verificação ou uma falha de <em>hardware</em> durante a execução. Entretanto, temos como vantagem a ausência de redundância na implementação da funcionalidade: apenas o <em>host</em> precisa implementá-la, simplificando o projeto, implementação e manutenção da rede.</p>
<p>Repare que o Argumento Fim-a-Fim é uma <strong>filosofia geral de projeto</strong>, mas não uma verdade absoluta. Conforme veremos nessa disciplina — e em Redes de Computadores II —, é possível encontrar uma série de exceções a essa filosofia em protocolos e mecanismos utilizados na Internet. Em geral, essas exceções estão ligadas a benefícios de desempenho ao se implementar determinado mecanismo na rede, ao invés de delegá-lo (totalmente) as pontas da comunicação. Por exemplo, voltando à aplicação de transmissão de arquivos, um benefício de realizar a verificação de integridade salto a salto é que corrupções dos pacotes podem ser recuperadas mais rapidamente. Isso porque, no caso fim-a-fim, a detecção da corrupção e solicitação de uma nova cópia do pacote só acontecem quando esse chega ao destinatário, o que faz com que o tempo total para a versão íntegra chegar também aumente. Além disso, <strong>recursos da rede podem ser desperdiçados</strong> com a transmissão por vários enlaces de um pacote corrompido anteriormente.</p>
<p>Assim, em certos <strong>casos particulares</strong> otimizações são realizadas em desacordo com o Argumento Fim-a-Fim. No entanto, o Argumento Fim-a-Fim é majoritariamente seguido no projeto arquitetural e de protocolos da Internet. Veremos inúmeros exemplos disso ao longo desse curso.</p>
<h3 id="argumento-fim-a-fim-analogias">Argumento Fim-a-Fim: Analogias</h3>
<p>Embora o argumento Fim-a-Fim seja uma filosofia de projeto aplicável a redes de comunicação, é possível encontrar paralelos com conceitos aplicados em outras áreas. Por exemplo, na área de arquitetura de computadores, há o embate entre arquiteturas RISC (<em>Reduced Instruction Set Computer</em>) e CISC (<em>Complex Instruction Set Computer</em>). Uma arquitetura CISC implementa um conjunto grande de instruções de máquina, suprindo, assim, em <em>hardware</em> grande parte das operações tipicamente realizadas pelas aplicações. Já uma arquitetura RISC oferece um conjunto de instruções comparativamente menor, focando em oferecer um mínimo de operações em <em>hardware</em> que sejam suficientes para que as funcionalidades mais complexas sejam implementadas em <em>sofware</em>. Embora fuja ao escopo dessa disciplina discutir os méritos de cada abordagem, os defensores de arquiteturas RISC argumentam que o menor conjunto de instruções torna os processadores mais simples, menos susceptíveis a <em>bugs</em> e pode trazer benefícios de desempenho ao permitir implementações mais eficientes das instruções básicas oferecidas. Tais argumentos são bem similares aos usados para justificar o argumento Fim-a-Fim.</p>
<p>Outro exemplo é o princípio da Navalha de Occam. Esse princípio filosófico defende que, dadas duas explicações consistentes para um mesmo fenômeno, é preferível optar pela mais simples — <em>i.e.</em>, a que contém o menor número de suposições. Esse princípio é frequentemente utilizado em ciência para a explicação de fenômenos observados. Uma das razões para isso é ideia intuitiva de que, quanto menor o número de hipóteses não testadas em uma explicação para um dado fenômeno, menor a probabilidade de que essa explicação venha a ser refutada futuramente — por exemplo, por novas observações inconsistentes com a explicação ou através de contraprovas às hipóteses. Assim como no caso do Argumento Fim-a-Fim, opta-se pela simplicidade, em detrimento a complexidades adicionais injustificadas.</p>
<h3 id="argumento-fim-a-fim-inteligência-nas-bordas">Argumento Fim-a-Fim: Inteligência nas Bordas</h3>
<p>Um corolário do Argumento Fim-a-Fim é a tendência na Internet de se <strong>manter a complexidade nas bordas</strong>. Dito de outra forma, o projeto da Internet busca manter o <strong>núcleo da rede simples</strong>, ao concentrar a <strong>inteligência nas bordas</strong>.</p>
<p>Como os elementos do núcleo da rede tendem a processar um volume muito maior de pacotes que os da borda, objetiva-se mantê-los o mais simples e especializados o possível, de forma que eles possam exercer essas poucas funcionalidades de maneira bastante eficiente. Em última análise, ao manter o núcleo da rede simples, aumentamos a capacidade de <strong>escalabilidade</strong> da Internet — em termos de suportar demandas crescentes de usuários e tráfego. Já nas bordas, como o volume de pacotes processados é tipicamente menor, pode-se dar ao luxo de realizar tarefas mais complexas, já que a pressão por escalabilidade é mais baixa.</p>
<p>Embora posto nesses termos o conceito de inteligência nas bordas possa parecer uma opção óbvia, vale destacar que nem toda rede de comunicação é assim. De fato, outra rede de comunicação extremamente popular adota o paradigma exatamente oposto: na telefonia fixa, o núcleo concentra a enorme maioria das funcionalidades, enquanto os terminais — aparelhos de telefone — são bastante simples.</p>
<h2 id="o-princípio-kiss">O Princípio KISS</h2>
<p>Outra filosofia geral de projeto amplamente adotada na Internet é o chamado <em>princípio KISS</em> (do inglês, <em>Keep It Simple, Stupidy!</em>). Esse princípio foi originado na Marinha Americana na década de 1960, mas foi incorporado às filosofias que guiam o desenvolvimento da Internet desde os seus primórdios. O princípio KISS defende que sistemas <em>funcionam melhor</em> quando são mantidos <strong>simples</strong>. Partindo dessa premissa, a simplicidade deve ser um objetivo do projeto. Assim, quaisquer complexidades que não sejam absolutamente necessárias aos requisitos do sistema deve ser evitadas.</p>
<p>O princípio KISS é aplicável a qualquer projeto de computação ou engenharia. No caso particular da Internet, ele se manifesta principalmente na forma de projetos de protocolos simples (relativamente falando). Quanto mais simples é um protocolo, mais fácil é sua implementação, depuração e extensão. Ao longo de todo o curso, veremos exemplos de opções por simplicidade em diversos protocolos amplamente utilizados na Internet.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Embora existam várias exceções, conforme veremos ao longo deste curso.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Mecanismos de verificação de integridade de pacotes existem e são bastante utilizados em redes de computadores. Por ora, não entraremos em detalhes como eles funcionam. No terceiro capítulo desse curso veremos um exemplo.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
