<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Políticas de Enfileiramento</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Políticas de Enfileiramento</h1>
</header>
<p>Como parte do processo de encaminhamento de um pacote por um roteador, é comum que o pacote precise ser enfileirado até que o enlace de saída se torne disponível para transmissão. De fato, roteadores na Internet estão a todo momento gerenciando as filas de pacotes associadas a cada um dos seus enlaces. Esse gerenciamento contempla duas tarefas básicas:</p>
<ul>
<li><strong>Escalonamento</strong>: quando o enlace se torna ocioso, decidir qual dos pacotes enfileirados será o próximo a ser transmitido.</li>
<li><strong>Descarte</strong>: quando a fila está cheia e um novo pacote deve ser encaminhado pelo enlace correspondente, decidir qual pacote será descartado.</li>
</ul>
<p>Embora haja opções óbvias para como escolher os pacotes em ambos os casos, essas escolhas nem sempre são as melhores possíveis. Além disso, a forma pela qual um roteador gerencia suas filas tem impacto representativo no desempenho e comportamento da rede. Por esses motivos, nesse material cobriremos algumas possíveis políticas — também chamadas de <em>disciplinas</em> — de enfileiramento (escalonamento e descarte) e discutiremos seus potenciais benefícios e problemas.</p>
<h2 id="políticas-de-escalonamento">Políticas de Escalonamento</h2>
<p>Até aqui, nesta disciplina, sempre assumimos implicitamente uma política de escalonamento do tipo FIFO (<em>First-In First-Out</em>). Em outras palavras, assumimos que os pacotes entram no final da fila e sempre o primeiro pacote é escolhido para a próxima transmissão.</p>
<p>A política FIFO é, de fato, a mais comum e popular na Internet. Ela é simples e tende a dar oportunidades iguais a todos os pacotes, por respeitar a ordem na qual eles chegam ao roteador. Entretanto, em certas situações, o emprego de outras políticas de escalonamento pode trazer benefícios. Se temos, por exemplo, fluxos de melhor esforço (<em>e.g.</em>, fluxos TCP relativos a aplicações de transferência de arquivos) competindo com fluxos de tempo-real (<em>e.g.</em>, uma chamada VoIP), pode ser do interesse da rede que os pacotes da chamada VoIP “passem à frente” na fila.</p>
<p>Para implementar este tipo de funcionalidade, é preciso que o roteador seja capaz de <strong>diferenciar</strong> os pacotes de acordo com seus fluxos ou, ao menos, em classes de importância. Neste caso, antes de chegar à fila propriamente dita, o pacote passa por um elemento chamado de <strong>classificador</strong>. Como um classificador distingue a classe a qual pertence um pacote está além do escopo desta aula (na verdade, isso só será abordado em outras disciplinas). O fato é que uma vez classificados os pacotes, políticas de escalonamento cientes de prioridades podem ser utilizadas.</p>
<p>O exemplo mais simples de uma política deste tipo é a <em>Priority Queueing</em>. Esta política aloca <em>buffers</em> separados para cada classe. Quando um pacote chega à fila, portanto, ele é colocado no final do <em>buffer</em> da sua classe específica. Cada classe possui uma prioridade e, quando o próximo pacote a ser transmitido deve ser escolhido, a <em>priority queueing</em> percorre os <em>buffers</em> da classe de maior prioridade para a de menor prioridade: se houver um pacote no <em>buffer</em> da classe de maior prioridade, este é escolhido para transmissão; caso contrário, consulta-se o <em>buffer</em> da próxima classe com a maior prioridade entre as restantes.</p>
<p>Nota-se, portanto, que, enquanto houver pacotes nos <em>buffers</em> das classes de maior prioridade, as classes de menor prioridade não serão servidas. Isso efetivamente atinge o efeito desejado de deixar certos pacotes “furarem fila”. Por outro lado, pacotes de mais baixa prioridade podem ser preteridos indefinidamente, uma situação chamada de <em>starvation</em> (esfomeação ou inanição em português).</p>
<p>O <em>starvation</em> pode ser combatido com outra política de escalonamento: a <em>round-robin</em>. Assim como na política <em>priority queueing</em>, pacotes são divididos em classes e cada classe possui seu próprio <em>buffer</em>. Quando o próximo pacote a ser transmitido precisa ser escolhido, a política <em>Round-Robin</em> percorre as filas em ordem, <strong>a partir da fila logo após aquela da qual o último pacote transmitido foi tirado</strong>. Em outras palavras, a cada nova oportunidade de transmissão, a política <em>round-robin</em> dá chance para uma nova classe. Isso significa que todas as classes receberão serviço, independentemente do que ocorre nas demais. Embora a política <em>round-robin</em> não gere <em>starvation</em>, ela também não fornece nenhum tipo de prioridade a uma classe sobre outra — ao contrário, ela garante que os recursos dedicados a uma classe não interferirão com os recursos das demais classes, seja espaço em <em>buffer</em>, seja tempo de transmissão.</p>
<p>Uma política que consegue, simultaneamente, garantir priorização de classes e evitar <em>starvation</em> é a <em>Weighted-Fair Queueing</em> (ou WFQ). A WFQ atribui prioridades numéricas que são proporcionais à fração do tempo de utilização do enlace que será dedicada a cada classe. Assim, classes de maior prioridade terão acesso a uma fração maior do tempo de uso do enlace. Por outro lado, classes de prioridade mais baixa ainda receberão <em>alguma</em> fração não-nula de tempo para utilizar o enlace, independentemente da ocupação dos <em>buffers</em> das classes de mais alta prioridade.</p>
<p>Uma forma de implementar a WFQ com pesos inteiros positivos e fazer uma ligeira alteração em como a política <em>round-robin</em> opera. Assim como na <em>round-robin</em>, as filas das várias classes são percorridas ciclicamente à medida que o enlace se torna disponível para transmissão. Entretanto, ao invés de transmitir um único pacote de uma classe e passar para a próxima, o WFQ transmite <span class="math inline">\(w\)</span> pacotes da classe atual antes de passar para a seguinte, onde <span class="math inline">\(w\)</span> é o valor do peso atribuído. Nessa implementação, uma classe <span class="math inline">\(A\)</span> que tenha sido configurada com peso <span class="math inline">\(w_A\)</span> receberá uma fração <span class="math inline">\(\frac{w_A}{W}\)</span>, onde <span class="math inline">\(W\)</span> denota o somatório dos pesos de todas as classes.</p>
<h2 id="políticas-de-descarte-de-pacotes">Políticas de Descarte de Pacotes</h2>
<p>A política de descarte de pacotes mais simples e comum é chamada de <em>Drop-tail</em>. Quando o <em>buffer</em> está cheio e um novo pacote chega, a política <em>Drop-tail</em> simplesmente descarta o pacote recém-recebido.</p>
<p>Embora esta abordagem seja intuitiva, a <em>Drop-tail</em> pode resultar em alguns problemas. Um destes problemas é a possibilidade de <strong>sincronização de fluxos</strong>. Suponha que dois <em>hosts</em> conectados a um mesmo roteador originam um fluxo TCP cada um, compartilhando um enlace de saída — e, portanto, sua fila. Como já discutido anteriormente, o TCP tem por característica a geração de tráfego em rajada. Suponha que um dos dois <em>hosts</em> realiza a transmissão de uma rajada de pacotes do tamanho do <em>buffer</em> disponível na porta de saída do roteador. Em seguida, o outro <em>host</em> transmite a sua rajada de pacotes. Supondo que o enlace de saída do roteador seja o gargalo, quando os pacotes da rajada gerada pelo segundo <em>host</em> chegam ao <em>buffer</em>, este já está totalmente ocupado por pacotes da rajada do primeiro <em>host</em>. Logo, em uma política do tipo <em>Drop-tail</em> os pacotes da rajada do segundo <em>host</em> poderiam ser todos descartados em sequência. Em última análise, este comportamento causa um compartilhamento injusto dos recursos da rede.</p>
<p>Uma política de descarte alternativa, porém ainda bastante simples, é a <em>Drop-head</em>. Como o nome sugere, quando um novo pacote chega ao <em>buffer</em> e este se encontra cheio, descarta-se o primeiro pacote da fila — ou seja, o pacote que está a mais tempo enfileirado. Uma análise superficial da política <em>Drop-head</em> pode sugerir que ela é injusta. Afinal, estamos descartando um pacote que está no início da fila e, portanto, muito próximo a ser transmitido.</p>
<p>No entanto, a motivação para esta decisão se torna clara quando consideramos fluxos TCP. Como já estudado nesta disciplina, o TCP infere congestionamento através da detecção de segmentos perdidos. Esta detecção, por sua vez, acontece pelo recebimento de <em>acks</em> duplicados sucessivos ou pelo estouro de temporizador. Em ambos os casos, as detecções de perda do TCP sempre são realizadas sobre os segmentos mais antigos atualmente em trânsito. Voltando à política <em>Drop-head</em>, em uma situação de congestionamento, se temos a opção entre descartar um dentre potencialmente vários segmentos TCP, pode ser interessante descartar o segmento mais antigo, já que os eventos que indicam perda de pacote para o TCP tenderão a ocorrer em um futuro mais próximo para o segmento que foi transmitido há mais tempo que para um segmento mais recente. Colocando de outra forma, se não descartarmos o segmento que está no início da fila — depois de ter sofrido um alto atraso de enfileiramento no <em>buffer</em> do nosso roteador — corremos o “risco” que ele chegue ao seu destinatário final e que o <em>ack</em> correspondente volte ao transmissor antes de eventos como <em>acks</em> duplicados ou estouro de temporizador ocorram e indiquem o congestionamento ao transmissor TCP. Desta forma, descartar o segmento no início do <em>buffer</em> pode ter o efeito desejável de avisar de maneira antecipada ao TCP sobre o congestionamento.</p>
<p>Note, no entanto, que o problema de sincronização de fluxos que afeta a política <em>Drop-tail</em> também pode se manifestar na política <em>Drop-head</em>, embora de forma invertida. Se repetirmos a análise do cenário com os dois <em>hosts</em> compartilhando um enlace de saída de um roteador e gerando tráfego em rajadas, concluiremos, novamente, que um dos <em>hosts</em> será consistentemente prejudicando, tendo seus pacotes descartados em benefício dos pacotes do outro fluxo. Desta vez, no entanto, por descartamos os pacotes do início da fila, o fluxo prejudicado será o do primeiro <em>host</em> a transmitir: à medida que os pacotes da rajada do segundo <em>host</em> chegam ao roteador e encontram a fila cheia, os pacotes que já estavam na fila, <em>i.e.</em>, os do primeiro <em>host</em>, serão descartados um a um.</p>
<p>A terceira e última política de descarte que estudaremos nesta disciplina é chamada de RED (do inglês <em>Random Early Detection</em> ou <em>Random Early Drop</em>). Esta política se diferencia das duas outras políticas estudadas nesta aula em vários aspectos. Talvez o aspecto mais marcante da política RED é o fato de que ela pode determinar o descarte de pacotes <strong>mesmo se a fila não se encontra totalmente cheia</strong>.</p>
<p>A política RED utiliza uma série de parâmetros para balizar seu funcionamento. De forma simplificada, estes parâmetros incluem:</p>
<ol type="1">
<li><p><strong>Mínimo</strong>. Determina um limite inferior de ocupação da fila a partir do qual pacotes <em>podem</em> ser descartados.</p></li>
<li><p><strong>Máximo</strong>. Determina um limite superior de ocupação da fila até o qual pacotes <em>podem</em> <strong>não</strong> ser descartados.</p></li>
</ol>
<p>Quando um novo pacote chega ao <em>buffer</em>, a política RED compara a ocupação atual da fila. Se a fila está atualmente menor que o valor do parâmetro <code>Mínimo</code>, o novo pacote é sempre aceito. Se a fila está atualmente maior ou igual ao valor do parâmetro <code>Máximo</code>, o pacote recém-recebido é <strong>sempre</strong> descartado. Por outro lado, se a ocupação atual da fila se encontra entre os valores dos parâmetros <code>Mínimo</code> e <code>Máximo</code>, o pacote recém-recebido é descartado com uma probabilidade <span class="math inline">\(p\)</span> que é proporcional a esta ocupação (a probabilidade cresce linearmente entre 0 e 1 à medida que a ocupação varia entre <code>Mínimo</code> e <code>Máximo</code>).</p>
<p>A motivação usada pelo RED para descartar pacotes antes do <em>buffer</em> estar completamente cheio é similar à utilizada para explicar os potenciais benefícios da política <em>Drop-head</em>: o objetivo é alertar antecipadamente as fontes de tráfego sobre a situação de congestionamento. Lembre-se que os descartes de pacotes são uma consequência de uma situação <strong>prolongada</strong> — e extrema — de congestionamento. Quando a ocupação do <em>buffer</em> de uma porta de saída é <em>representativa</em>, isso já sinaliza que existe congestionamento. Esta ocupação representativa é determinada no RED através do parâmetro <code>Mínimo</code>. Logo, a partir deste nível de enfileiramento, o RED começa a descartar pacotes — ainda que probabilisticamente — com o intuito de alertar as fontes que estas devem reduzir suas taxas de transmissão.</p>
<p>De fato, quando configurado de forma correta, o RED permite que o TCP convirja para a vazão máxima do caminho, mantendo atrasos relativamente baixos — em comparação aos obtidos por outras disciplinas de enfileiramento. Um problema do RED é que nem sempre é fácil encontrar valores adequados para seus parâmetros de acordo com o cenário em questão. Isso dificulta uma adoção mais ampla desta técnica.</p>
<p>Um efeito colateral interessante do RED é a solução do problema do sincronismo. Repare que, embora o RED sempre descarte o pacote recém-recebido, ele não o faz apenas quando o <em>buffer</em> está cheio. Logo, de certa maneira, o pacote escolhido para descarte no RED pode ser considerado aleatório, ao contrário do que ocorre nas políticas <em>Drop-tail</em> e <em>Drop-head</em>. Esta natureza aleatória do RED faz com que ele não prejudique consistentemente um mesmo fluxo, distribuindo seus descartes de maneira homogênea entre os vários fluxos que compartilham o enlace.</p>
</body>
</html>
